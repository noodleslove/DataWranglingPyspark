{"cells":[{"cell_type":"markdown","id":"50e4e10a-82d1-4840-a207-f4e64a7bd20e","metadata":{},"source":["![ecommerce_analytics-1224x532](ecommerce_analytics-1224x532.jpg)\n"]},{"cell_type":"markdown","id":"954c11f8-0b2d-4883-95d7-4b5ff4aee339","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"source":["As a Data Engineer at an electronics e-commerce company, Voltmart, you have been requested by a peer Machine Learning team to clean the data containing the information about orders made last year. They are planning to further use this cleaned data to build a demand forecasting model. To achieve this, they have shared their requirements regarding the desired output table format.\n","\n","An analyst shared a parquet file called `\"orders_data.parquet\"` for you to clean and preprocess. \n","\n","You can see the dataset schema below along with the **cleaning requirements**:"]},{"cell_type":"markdown","id":"086e2bd3-ff58-423b-98e0-a7e4c979d7f3","metadata":{},"source":["## `orders_data.parquet`\n","\n","| column | data type | description | cleaning requirements | \n","|--------|-----------|-------------|-----------------------|\n","| `order_date` | `timestamp` | Date and time when the order was made | _Modify: Remove orders placed between 12am and 5am (inclusive); convert from timestamp to date_ |\n","| `time_of_day` | `string` | Period of the day when the order was made | _New column containing (lower bound inclusive, upper bound exclusive): \"morning\" for orders placed 5-12am, \"afternoon\" for orders placed 12-6pm, and \"evening\" for 6-12pm_ |\n","| `order_id` | `long` | Order ID | _N/A_ |\n","| `product` | `string` | Name of a product ordered | _Remove rows containing \"TV\" as the company has stopped selling this product; ensure all values are lowercase_ |\n","| `product_ean` | `double` | Product ID | _N/A_ |\n","| `category` | `string` | Broader category of a product | _Ensure all values are lowercase_ |\n","| `purchase_address` | `string` | Address line where the order was made (\"House Street, City, State Zipcode\") | _N/A_ |\n","| `purchase_state` | `string` | US State of the purchase address | _New column containing: the State that the purchase was ordered from_ |\n","| `quantity_ordered` | `long` | Number of product units ordered | _N/A_ |\n","| `price_each` | `double` | Price of a product unit | _N/A_ |\n","| `cost_price` | `double` | Cost of production per product unit | _N/A_ |\n","| `turnover` | `double` | Total amount paid for a product (quantity x price) | _N/A_ |\n","| `margin` | `double` | Profit made by selling a product (turnover - cost) | _N/A_ |\n","\n","<br>"]},{"cell_type":"code","execution_count":1,"id":"62b55d87-f8fd-4e8a-9ef6-6fe91f4de5f3","metadata":{"executionCancelledAt":null,"executionTime":4755,"lastExecutedAt":1729292067229,"lastExecutedByKernel":"ef777fe8-782a-4e4a-bdb8-544379791f4a","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from pyspark.sql import (\n    SparkSession,\n    types,\n    functions as F,\n)\n\nspark = (\n    SparkSession\n    .builder\n    .appName('cleaning_orders_dataset_with_pyspark')\n    .getOrCreate()\n)","outputsMetadata":{"0":{"height":101,"type":"stream"},"1":{"height":59,"type":"stream"}}},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/10/18 22:54:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"]}],"source":["from pyspark.sql import (\n","    SparkSession,\n","    types,\n","    functions as F,\n",")\n","\n","spark = (\n","    SparkSession\n","    .builder\n","    .appName('cleaning_orders_dataset_with_pyspark')\n","    .getOrCreate()\n",")"]},{"cell_type":"code","execution_count":2,"id":"fa903d28-82b2-4c39-90b1-3d9a9421fb6d","metadata":{"executionCancelledAt":null,"executionTime":7260,"lastExecutedAt":1729292077535,"lastExecutedByKernel":"ef777fe8-782a-4e4a-bdb8-544379791f4a","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"orders_data = spark.read.parquet('orders_data.parquet')\norders_data.toPandas().head()","outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":237,"type":"dataFrame"}}},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"application/com.datacamp.data-table.v2+json":{"table":{"data":{"category":["Vêtements","Alimentation","Vêtements","Sports","Électronique"],"cost_price":[231,7.475,5.995,97.4935,5.995],"index":[0,1,2,3,4],"margin":[469,7.475,11.99,52.4965,5.995],"order_date":["2023-01-22T21:25:00.000","2023-01-28T14:15:00.000","2023-01-17T13:33:00.000","2023-01-05T20:33:00.000","2023-01-25T11:59:00.000"],"order_id":[141234,141235,141236,141237,141238],"price_each":[700,14.95,11.99,149.99,11.99],"product":["iPhone","Lightning Charging Cable","Wired Headphones","27in FHD Monitor","Wired Headphones"],"product_id":[5638008983335,5563319511488,2113973395220,3069156759167,9692680938163],"purchase_address":["944 Walnut St, Boston, MA 02215","185 Maple St, Portland, OR 97035","538 Adams St, San Francisco, CA 94016","738 10th St, Los Angeles, CA 90001","387 10th St, Austin, TX 73301"],"quantity_ordered":[1,1,2,1,1],"turnover":[700,14.95,23.98,149.99,11.99]},"schema":{"fields":[{"name":"index","type":"integer"},{"name":"order_date","type":"datetime"},{"name":"order_id","type":"integer"},{"name":"product","type":"string"},{"name":"product_id","type":"number"},{"name":"category","type":"string"},{"name":"purchase_address","type":"string"},{"name":"quantity_ordered","type":"integer"},{"name":"price_each","type":"number"},{"name":"cost_price","type":"number"},{"name":"turnover","type":"number"},{"name":"margin","type":"number"}],"pandas_version":"1.4.0","primaryKey":["index"]}},"total_rows":5,"truncation_type":null},"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>order_date</th>\n","      <th>order_id</th>\n","      <th>product</th>\n","      <th>product_id</th>\n","      <th>category</th>\n","      <th>purchase_address</th>\n","      <th>quantity_ordered</th>\n","      <th>price_each</th>\n","      <th>cost_price</th>\n","      <th>turnover</th>\n","      <th>margin</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2023-01-22 21:25:00</td>\n","      <td>141234</td>\n","      <td>iPhone</td>\n","      <td>5.638009e+12</td>\n","      <td>Vêtements</td>\n","      <td>944 Walnut St, Boston, MA 02215</td>\n","      <td>1</td>\n","      <td>700.00</td>\n","      <td>231.0000</td>\n","      <td>700.00</td>\n","      <td>469.0000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2023-01-28 14:15:00</td>\n","      <td>141235</td>\n","      <td>Lightning Charging Cable</td>\n","      <td>5.563320e+12</td>\n","      <td>Alimentation</td>\n","      <td>185 Maple St, Portland, OR 97035</td>\n","      <td>1</td>\n","      <td>14.95</td>\n","      <td>7.4750</td>\n","      <td>14.95</td>\n","      <td>7.4750</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2023-01-17 13:33:00</td>\n","      <td>141236</td>\n","      <td>Wired Headphones</td>\n","      <td>2.113973e+12</td>\n","      <td>Vêtements</td>\n","      <td>538 Adams St, San Francisco, CA 94016</td>\n","      <td>2</td>\n","      <td>11.99</td>\n","      <td>5.9950</td>\n","      <td>23.98</td>\n","      <td>11.9900</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2023-01-05 20:33:00</td>\n","      <td>141237</td>\n","      <td>27in FHD Monitor</td>\n","      <td>3.069157e+12</td>\n","      <td>Sports</td>\n","      <td>738 10th St, Los Angeles, CA 90001</td>\n","      <td>1</td>\n","      <td>149.99</td>\n","      <td>97.4935</td>\n","      <td>149.99</td>\n","      <td>52.4965</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2023-01-25 11:59:00</td>\n","      <td>141238</td>\n","      <td>Wired Headphones</td>\n","      <td>9.692681e+12</td>\n","      <td>Électronique</td>\n","      <td>387 10th St, Austin, TX 73301</td>\n","      <td>1</td>\n","      <td>11.99</td>\n","      <td>5.9950</td>\n","      <td>11.99</td>\n","      <td>5.9950</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           order_date  order_id  ... turnover    margin\n","0 2023-01-22 21:25:00    141234  ...   700.00  469.0000\n","1 2023-01-28 14:15:00    141235  ...    14.95    7.4750\n","2 2023-01-17 13:33:00    141236  ...    23.98   11.9900\n","3 2023-01-05 20:33:00    141237  ...   149.99   52.4965\n","4 2023-01-25 11:59:00    141238  ...    11.99    5.9950\n","\n","[5 rows x 11 columns]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["orders_data = spark.read.parquet('orders_data.parquet')\n","orders_data.toPandas().head()"]},{"cell_type":"code","execution_count":3,"id":"3840eb0c-459a-4ba7-b344-028c6aaaae41","metadata":{"executionCancelledAt":null,"executionTime":4903,"lastExecutedAt":1729292084927,"lastExecutedByKernel":"ef777fe8-782a-4e4a-bdb8-544379791f4a","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from pyspark.sql import (\n    SparkSession,\n    types,\n    functions as F,\n)\n\n# Initiate a Spark session\nspark = (\n    SparkSession\n    .builder\n    .appName('cleaning_orders_dataset_with_pyspark')\n    .getOrCreate()\n)\n\n# IMPORT DATA\n\n# Read data from the parquet file\norders_data = spark.read.parquet('orders_data.parquet')\n\n# DATA CLEANING AND PREPROCESSING\n\norders_data = (\n    orders_data\n    # Create a new column time_of_day\n    .withColumn(\n        'time_of_day',\n        # When/otherwise (similar to case/when/else) statements extracting hour from timestamp\n        F.when((F.hour('order_date') >= 0) & (F.hour('order_date') <= 5), 'night')\n         .when((F.hour('order_date') >= 6) & (F.hour('order_date') <= 11), 'morning')\n         .when((F.hour('order_date') >= 12) & (F.hour('order_date') <= 17), 'afternoon')\n         .when((F.hour('order_date') >= 18) & (F.hour('order_date') <= 23), 'evening')\n        # You can keep the otherwise statement as None to validate whether the conditions are exhaustive\n         .otherwise(None)\n    )\n    # Filter by time of day\n    .filter(\n        F.col('time_of_day') != 'night'\n    )\n    # Cast order_date to date as it is originally a timestamp\n    .withColumn(\n        'order_date',\n        F.col('order_date').cast(types.DateType())\n    )\n)\n\n\norders_data = (\n    orders_data\n    # Make product and category columns lowercase\n    .withColumn(\n        'product',\n        F.lower('product')\n    )\n    .withColumn(\n        'category',\n        F.lower('category')\n    )\n    # Remove rows where product column contains \"tv\" (as you have already made it lowercase)\n    .filter(\n        ~F.col('product').contains('tv')\n    )\n)\n\n\norders_data = (\n    orders_data\n    # First you split the purchase address by space (\" \")\n    .withColumn(\n        'address_split',\n        F.split('purchase_address', ' ')\n    )\n    # If you look at the address lines, you can see that the state abbreviation is always at the 2nd last position\n    .withColumn(\n        'purchase_state',\n        F.col('address_split').getItem(F.size('address_split') - 2)\n    )\n    # Dropping address_split columns as it is a temporary technical column\n    .drop('address_split')\n)\n\n# Use distinct and count to calculate the number of unique values\nn_states = (\n    orders_data\n    .select('purchase_state')\n    .distinct()\n    .count()\n)\n\n\n# EXPORT\n\n# Export the resulting table to parquet format with the new name\n(\n    orders_data\n    .write\n    .parquet(\n        'orders_data_clean.parquet',\n        mode='overwrite',\n    )\n)","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# IMPORT DATA\n","\n","# Read data from the parquet file\n","orders_data = spark.read.parquet('orders_data.parquet')\n","\n","# DATA CLEANING AND PREPROCESSING\n","\n","orders_data = (\n","    orders_data\n","    # Create a new column time_of_day\n","    .withColumn(\n","        'time_of_day',\n","        # When/otherwise (similar to case/when/else) statements extracting hour from timestamp\n","        F.when((F.hour('order_date') >= 0) & (F.hour('order_date') <= 5), 'night')\n","         .when((F.hour('order_date') >= 6) & (F.hour('order_date') <= 11), 'morning')\n","         .when((F.hour('order_date') >= 12) & (F.hour('order_date') <= 17), 'afternoon')\n","         .when((F.hour('order_date') >= 18) & (F.hour('order_date') <= 23), 'evening')\n","        # You can keep the otherwise statement as None to validate whether the conditions are exhaustive\n","         .otherwise(None)\n","    )\n","    # Filter by time of day\n","    .filter(\n","        F.col('time_of_day') != 'night'\n","    )\n","    # Cast order_date to date as it is originally a timestamp\n","    .withColumn(\n","        'order_date',\n","        F.col('order_date').cast(types.DateType())\n","    )\n",")\n","\n","\n","orders_data = (\n","    orders_data\n","    # Make product and category columns lowercase\n","    .withColumn(\n","        'product',\n","        F.lower('product')\n","    )\n","    .withColumn(\n","        'category',\n","        F.lower('category')\n","    )\n","    # Remove rows where product column contains \"tv\" (as you have already made it lowercase)\n","    .filter(\n","        ~F.col('product').contains('tv')\n","    )\n",")\n","\n","\n","orders_data = (\n","    orders_data\n","    # First you split the purchase address by space (\" \")\n","    .withColumn(\n","        'address_split',\n","        F.split('purchase_address', ' ')\n","    )\n","    # If you look at the address lines, you can see that the state abbreviation is always at the 2nd last position\n","    .withColumn(\n","        'purchase_state',\n","        F.col('address_split').getItem(F.size('address_split') - 2)\n","    )\n","    # Dropping address_split columns as it is a temporary technical column\n","    .drop('address_split')\n",")\n","\n","# Use distinct and count to calculate the number of unique values\n","n_states = (\n","    orders_data\n","    .select('purchase_state')\n","    .distinct()\n","    .count()\n",")\n","\n","\n","# EXPORT\n","\n","# Export the resulting table to parquet format with the new name\n","(\n","    orders_data\n","    .write\n","    .parquet(\n","        'orders_data_clean.parquet',\n","        mode='overwrite',\n","    )\n",")\n","\n","# Stop the Spark session\n","spark.stop()"]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"editor":"DataLab","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}
